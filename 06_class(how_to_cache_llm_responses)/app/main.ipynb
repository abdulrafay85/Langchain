{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain me caching ka matlab yeh hota hai ke agar aap ek hi sawal ya query bar-bar pochte hain, to LLM (Language Model) har baar naye se response banane ke bajaye pehle wala jawab (jo cache me save ho chuka hai) de dega. Isse time bachta hai aur agar aap paid API use kar rahe ho to paisay bhi bachte hain.\n",
    "\n",
    "Ab aapko 3 tarikon ka batata hoon jo LangChain me caching ke liye use hote hain:\n",
    "\n",
    "##### 1. **`set_llm_cache()`**\n",
    "Yeh ek function hai jo aapko cache set karne me madad karta hai. Iska kaam yeh hota hai ke jab aap yeh function call karte hain, to aap LLM ko batate hain ke wo responses kaha store kare (cache me). Jaise agar aapko in-memory ya database me store karna ho, to aap isko usi ke according setup kar sakte hain.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from langchain.cache import set_llm_cache, InMemoryCache\n",
    "\n",
    "# In-memory cache set kar rahe hain\n",
    "set_llm_cache(InMemoryCache())\n",
    "```\n",
    "Yeh aapko batata hai ke hum cache set kar rahe hain jahan LLM responses RAM (memory) me store honge.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **`InMemoryCache()`**\n",
    "Yeh cache ka ek type hai jo system ki memory (RAM) me data save karta hai. Iska fayda yeh hai ke yeh bohot tez hota hai, lekin yeh temporary hota hai. Matlab, agar program band ho jaye to cache ka data khatam ho jata hai. Isliye, yeh tab best hota hai jab aapko sirf program ke run hone tak data yaad rakhna ho.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# In-memory cache banaya\n",
    "cache = InMemoryCache()\n",
    "```\n",
    "Isme aapka data tab tak store hoga jab tak aapka program chal raha hai.\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **`SQLiteCache()`**\n",
    "Yeh cache ka doosra type hai jo responses ko disk pe ek database (SQLite) me store karta hai. Iska fayda yeh hai ke yeh data permanent hota hai, yani program band hone ke baad bhi save rehta hai. Yeh `InMemoryCache` se thoda slow hota hai lekin jab aapko data ko bar-bar use karna ho to yeh zyada useful hota hai.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "# SQLite database me cache banaya\n",
    "sqlite_cache = SQLiteCache(database_path=\".langchain_cache.db\")\n",
    "```\n",
    "Isme aapka data ek local database me save hoga, to agle session me bhi woh data wahan hoga.\n",
    "\n",
    "---\n",
    "\n",
    "##### Yeh cheezein kyun use hoti hain?\n",
    "\n",
    "- **API cost kam karna**: Agar aap bar-bar same query pochte hain, to aap dobara se API ka paisa dene se bach jate hain kyun ke result already saved hota hai.\n",
    "- **Response time fast karna**: Cache se result lene me kam waqt lagta hai.\n",
    "- **Data ko save rakhna**: Agar aap chahte hain ke data program band hone ke baad bhi save rahe, to aap `SQLiteCache` use kar sakte hain.\n",
    "\n",
    "Yeh 3 tarike aapko caching karne ke liye madad dete hain, taake aapki LLM queries fast aur efficient ho jayein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set_llm_cache()\n",
    "**`set_llm_cache()`** LangChain ka ek function hai jo aapko caching system set karne me madad karta hai. Iska kaam yeh hota hai ke jab aap koi request (sawal) LLM se karte hain, to uska jawab (response) cache me store kar liya jata hai. Agar aap same request dobara karein, to system naye se jawab generate karne ke bajaye pehle se stored (cached) jawab de dega. Isse 2 bade fayde hote hain:\n",
    "\n",
    "1. **API cost bachti hai**: Har baar LLM ko query karna mehnga pad sakta hai, lekin caching ki wajah se aapko bar-bar same response ke liye paisa nahi dena padta.\n",
    "2. **Time bachta hai**: Cache se result lene me LLM ko query karne ke mukable kam time lagta hai. To response jaldi milta hai.\n",
    "\n",
    "### `set_llm_cache()` kaise kaam karta hai?\n",
    "\n",
    "Jab aap LangChain me `set_llm_cache()` function ko use karte hain, aap batate hain ke aapka data kaha aur kis tarah store hoga. Aap cache ko alag-alag tarikon se setup kar sakte hain, jaise:\n",
    "\n",
    "- **Memory me**: Agar aap chahte hain ke response sirf program ke chalne tak yaad rakha jaye.\n",
    "- **Disk pe (Database me)**: Agar aapka response program band hone ke baad bhi save rahe, to aap disk pe cache set kar sakte hain.\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from langchain.cache import set_llm_cache, InMemoryCache\n",
    "\n",
    "# In-memory cache ko set kar rahe hain (RAM me data store hoga)\n",
    "set_llm_cache(InMemoryCache())\n",
    "```\n",
    "\n",
    "Yeh code example batata hai ke hum ek cache set kar rahe hain jahan responses ko memory (RAM) me save kiya jaye. Jab tak program chal raha hai, cache me save jawab bar-bar use hoga.\n",
    "\n",
    "### Aap yeh kyu use karte hain?\n",
    "\n",
    "1. **Bar-bar ek hi query pe paisa kharch nahi hota**: Jab aapka system same query pe already saved result de deta hai, to aapko dobara LLM ko query karne ka paisa nahi dena padta.\n",
    "  \n",
    "2. **Performance better hoti hai**: LLM se har baar naye result lene me time lagta hai, lekin cache me se result lene me kam time lagta hai. Isliye response tez milta hai.\n",
    "\n",
    "3. **Data consistency**: Jab aap same query karte hain, aapko consistent results milte hain kyun ke pehla response waisa ka waisa saved hota hai.\n",
    "\n",
    "---\n",
    "\n",
    "Aap caching ko alag-alag jagah use kar sakte hain, jaise agar aapko ek support system banana hai jahan user bar-bar same sawal pochte hain, to aap LLM ko dobara se query kiye bagair fast jawab de sakte hain cache ki madad se.\n",
    "\n",
    "Agar aapko aur details chahiye caching ke baray me ya koi aur sawal hai to pooch sakte hain!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
