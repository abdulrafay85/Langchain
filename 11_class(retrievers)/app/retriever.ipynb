{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever And Chain With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_compressor=LLMChainExtractor(llm_chain=PromptTemplate(input_variables=['context', 'question'], input_types={}, output_parser=NoOutputParser(), partial_variables={}, template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:')\n",
      "| ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000025C307F2A80>, default_metadata=())\n",
      "| NoOutputParser(), get_input=<function default_get_input at 0x0000025C0DCF2020>) base_retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025C30A5B740>, search_kwargs={})\n",
      "Answer: Here's how to cook Kabuli Pulao based on the provided recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 1 kg Sella rice\n",
      "* 1 kg boneless beef (cut into large pieces)\n",
      "* 3 medium onions\n",
      "* 5 medium tomatoes\n",
      "* 3 medium carrots\n",
      "* 3 serving spoons oil\n",
      "* 2 tbsp readymade garam masala\n",
      "* Salt to taste\n",
      "* 1 tbsp sugar\n",
      "* 2 tbsp raisins\n",
      "* 20 almonds\n",
      "* 5 walnuts\n",
      "* 10 cashew nuts\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Prepare the rice:** Soak the rice in water for 30 minutes before cooking.\n",
      "2. **Cook the beef:** Boil the beef in water with 1 tbsp of readymade garam masala and salt until tender (approximately 30 minutes in a cooker).\n",
      "3. **Saut√© onions and tomatoes:** In a pan, add oil and fry onions until brown. Then add tomatoes and fry.\n",
      "4. **Combine beef and sauce:** Add the cooked beef to the pan and fry for 5-10 minutes.\n",
      "5. **Add broth and spices:** Add the beef broth (double the amount of rice) to the pan.  Then add salt, 1 tbsp readymade garam masala, and raisins.\n",
      "6. **Add nuts and finish:** When some water is left, add all the nuts, cover, and let it simmer until the liquid is absorbed.\n",
      "\n",
      "\n",
      "The recipe doesn't specify cooking times for steps 3-6, or how to cook the rice after soaking.  It also omits carrots from the cooking instructions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Import necessary modules\n",
    "## Modules for document compressors, retrievers, and language model integration\n",
    "from langchain.retrievers.document_compressors import (\n",
    "    LLMChainFilter, LLMListwiseRerank, LLMChainExtractor, EmbeddingsFilter, DocumentCompressorPipeline\n",
    ")\n",
    "import uuid\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.storage import InMemoryByteStore, InMemoryStore\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter, LongContextReorder\n",
    "from langchain.retrievers import ContextualCompressionRetriever, ParentDocumentRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "\n",
    "# print(gemini_api_key)\n",
    "\n",
    "class DocumentTool:\n",
    "    def __init__(self, llm, embedding_model):\n",
    "        self.llm = llm\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.vectorstore = None\n",
    "\n",
    "    def load_documents(self, file_paths):\n",
    "        raw_documents = []\n",
    "        for path in file_paths:\n",
    "            if path.endswith(\".txt\"):\n",
    "                loader = TextLoader(path)\n",
    "            elif path.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(path)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Use .txt or .pdf\")\n",
    "            raw_documents += loader.load()\n",
    "        return raw_documents\n",
    "\n",
    "    def split_documents(self, raw_documents):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=20,\n",
    "            separators=[\"\\n\", \"\\n\\n\"]\n",
    "        )\n",
    "        return splitter.split_documents(raw_documents)\n",
    "\n",
    "    def create_vectorstore(self, docs):\n",
    "        self.vectorstore = FAISS.from_documents(docs, self.embeddings_model)\n",
    "        return self.vectorstore\n",
    "\n",
    "    def setup_retriever(self):\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vectorstore is not initialized. Call `create_vectorstore` first.\")\n",
    "        retriever = self.vectorstore.as_retriever()\n",
    "        compressor = LLMChainExtractor.from_llm(self.llm)\n",
    "        return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "    def ask_question(self, query, retriever):\n",
    "        compressed_docs = retriever.invoke(query)\n",
    "        qa_chain = load_qa_chain(self.llm, chain_type=\"stuff\")\n",
    "        return qa_chain.run({\"input_documents\": compressed_docs, \"question\": query})\n",
    "\n",
    "\n",
    "## Step 1: Initialize the Language Model (LLM)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  ## Model version\n",
    "    api_key=gemini_api_key,    ## API key for authentication\n",
    "    temperature=0.2            ## Control response randomness\n",
    ")\n",
    "\n",
    "\n",
    "## Step 3: Setup Embeddings Model for Similarity Matching\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=gemini_api_key,\n",
    "    model=\"models/text-embedding-004\"  ## Embedding model version\n",
    ")\n",
    "\n",
    "# Step 2: Create the tool instance\n",
    "tool = DocumentTool(llm=llm, embedding_model=embeddings_model)\n",
    "\n",
    "# Step 3: Load documents\n",
    "file_paths = [\"../files/example.txt\", \"../files/cocking.pdf\"]  # Add your file paths here\n",
    "raw_documents = tool.load_documents(file_paths)\n",
    "# print(raw_documents)\n",
    "\n",
    "# # Step 4: Split documents\n",
    "docs = tool.split_documents(raw_documents)\n",
    "# print(docs)\n",
    "\n",
    "# # Ste\n",
    "# p 5: Create vectorstore\n",
    "vectorstore = tool.create_vectorstore(docs)\n",
    "# print(vectorstore)\n",
    "\n",
    "# # Step 6: Setup retriever\n",
    "retriever = tool.setup_retriever()\n",
    "print(retriever)\n",
    "\n",
    "# # Step 7: Ask a question\n",
    "query = \"How to cook Kabuli Pulao?\"\n",
    "answer = tool.ask_question(query, retriever)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2606509741.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    function greet(name){\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
