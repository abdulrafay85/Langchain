{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rafay Khan\\anaconda3\\envs\\venv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API key from .env file\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI( \n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=gemini_api_key,\n",
    "    temperature=0.2,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. `file_path = \"../example.txt\"`\n",
    "   - **Explanation**: Is line mein aap ek file ka path define kar rahe hain. \n",
    "   - `file_path` variable mein ek relative path diya gaya hai jo `example.txt` file ko locate karta hai. \n",
    "   - `../` ka matlab hai ke current directory se ek directory peeche jaake `example.txt` ko search karega.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../Prompt Engineering for Generative AI Future-Proof Inputs for Reliable Al Outputs (James Phoenix, Mike Taylor).pdf\n"
     ]
    }
   ],
   "source": [
    "## PDF File Path\n",
    "file_path = \"../../../../Prompt Engineering for Generative AI Future-Proof Inputs for Reliable Al Outputs (James Phoenix, Mike Taylor).pdf\"\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. `loader = TextLoader(file_path)`\n",
    "   - **Explanation**: Yahaan pe aap `TextLoader` class ka instance bana rahe hain, jo `file_path` mein di gayi text file ko load karega.\n",
    "   - `TextLoader` LangChain mein ek document loader hai jo plain text files ko load karta hai. Yeh file ko read karta hai aur iske content ko `Document` objects mein convert karta hai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.text.TextLoader object at 0x000001D1FFBF6F60>\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(file_path)\n",
    "print(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chalo, isko aur simple tareeke se samjhata hoon.\n",
    "\n",
    "### 3. `mydata = loader.load()`\n",
    "\n",
    "#### Kya ho raha hai?\n",
    "Jab aap `load()` method call karte hain, to yeh `example.txt` file ko uthata hai, uske andar ka jo text hota hai wo read karta hai, aur is text ko ek special format mein store karta hai, jisko `Document` object kehte hain.\n",
    "\n",
    "#### `Document` Object kya hota hai?\n",
    "`Document` object ek special container hota hai jisme do main cheezen hoti hain:\n",
    "\n",
    "1. **`page_content`**:\n",
    "   - Yeh us file ka **main text** hota hai jo file ke andar likha hota hai. Example ke liye agar aapki file ke andar yeh likha ho:\n",
    "     ```\n",
    "     Hello, this is an example file.\n",
    "     ```\n",
    "     To `page_content` mein yeh pura text store hoga.\n",
    "\n",
    "2. **`metadata`**:\n",
    "   - Isme file se related **extra information** hoti hai, jaise:\n",
    "     - File ka naam (for example, `example.txt`).\n",
    "     - File ka path (for example, `../example.txt`).\n",
    "   - Aap file ke content ke ilawa kuch aur bhi track kar sakte ho, jaise kis page se data aa raha hai agar PDF hoti, ya file ka location.\n",
    "\n",
    "#### Jab aap `mydata = loader.load()` call karte hain:\n",
    "- File load hoti hai, aur jo text us file ke andar hota hai wo `page_content` ke andar store hota hai.\n",
    "- Saath mein file ka naam ya path `metadata` ke andar store hota hai.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = loader.load()\n",
    "print(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examplse = mydata[0].page_content\n",
    "print(examplse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFs w/ tables and Multi-Modal (text + images)\n",
    "\n",
    "Yeh pura process **PDFs ke analysis** ka ek structured tariqa batata hai, jisme tables aur images ko handle karne ke liye different approaches use ki jati hain. Tumhe yeh samjhane ki koshish karta hoon ke kis tarah se **Unstructured** library ko use karke PDFs se tables ko reliably extract kiya ja sakta hai, aur kaise tum multi-modal techniques (text + images) ka use karke PDF content ko aur bhi useful bana sakte ho.\n",
    "\n",
    "### Key Concepts Samjho:\n",
    "\n",
    "1. **PDF Tables Extraction**: \n",
    "   - Tables ko reliably extract karna traditional methods jaise character-based separators (e.g., comma ya tab) se mushkil hota hai. Is liye `Unstructured` jaise advanced tools ka use kiya jata hai jo tables ko accurately recognize aur extract kar sakein.\n",
    "   - **Unstructured** library ko use karke, tum tables ko HTML format mein extract karte ho jo LLMs (Language Models) ke liye easier to parse hota hai.\n",
    "\n",
    "2. **Unstructured Library**:\n",
    "   - **Unstructured** library LLMs ke liye data ko ready karne ke liye badi achi tool hai. Yeh tumhare PDFs ka content, specially **tables**, ko structure mein todne mein madad karti hai.\n",
    "   - Iska **high-resolution (hi_res)** strategy use hota hai tables ko detect karne ke liye, aur yeh **YOLOX model** ka use karti hai to understand bounding boxes for tables and embedded images.\n",
    "\n",
    "3. **Partitioning a PDF**:\n",
    "   - Tum `partition_pdf` function ka use karte ho taake PDF ko analyze kiya ja sake, aur tables, text, aur images ko recognize karke alag-alag extract kiya jaye.\n",
    "   - Tum **table structure infer** kar sakte ho, jo tables ko identify aur structure ko samajhne mein madad karta hai.\n",
    "\n",
    "4. **Example Code**:\n",
    "   ```python\n",
    "   from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "   # File path to your PDF\n",
    "   filename = \"static/SalesforceFinancial.pdf\"\n",
    "\n",
    "   # Partitioning the PDF\n",
    "   elements = partition_pdf(\n",
    "       filename=filename,\n",
    "       strategy=\"hi_res\",  # Using high-resolution strategy for better table extraction\n",
    "       infer_table_structure=True,  # Enable table structure recognition\n",
    "       model_name=\"yolox\"  # Using YOLOX model to detect table bounding boxes\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   - **Elements**: Yeh tumhare PDF ke parts hain jo alag-alag ho jate hain, jaise narrative text, tables, headings, etc.\n",
    "   - Tum tables ko HTML format mein bhi extract kar sakte ho, jo LLM ko easily process karne mein madad karta hai.\n",
    "\n",
    "   ```python\n",
    "   table_html = elements[-4].metadata.text_as_html\n",
    "   print(table_html)\n",
    "   ```\n",
    "\n",
    "5. **Tables and Semantic Search**:\n",
    "   - Jab tum tables ko extract kar lete ho, toh tum **semantic search** mein problem face kar sakte ho agar tum raw tables par embeddings match karne ki koshish karo. Is liye common practice yeh hoti hai ke tables ka ek **summary** generate kar liya jaye, aur fir us summary ka embedding create kiya jaye.\n",
    "\n",
    "6. **Multi-Modal (Text + Images)**:\n",
    "   - **Unstructured** tumhe PDF mein embedded images bhi extract karne ka option deti hai.\n",
    "   - Tum multi-modal techniques ka use karte ho jaise **GPT-4V** ko use karke images ka summary generate kar sakte ho. Yeh process images ko meaningful format mein convert karta hai jo tumhare LLM ko aur useful insights provide karta hai.\n",
    "\n",
    "   Example code for extracting images:\n",
    "   ```python\n",
    "   from PIL import Image\n",
    "   import base64\n",
    "   import io\n",
    "\n",
    "   # Function to convert image to base64 format\n",
    "   def image_to_base64(image_path):\n",
    "       with Image.open(image_path) as image:\n",
    "           buffered = io.BytesIO()\n",
    "           image.save(buffered, format=image.format)\n",
    "           img_str = base64.b64encode(buffered.getvalue())\n",
    "           return img_str.decode('utf-8')\n",
    "\n",
    "   image_str = image_to_base64(\"static/pdfImages/figure-15-6.jpg\")\n",
    "   ```\n",
    "\n",
    "   Fir tum GPT-4V ka use karke image ka summary generate kar sakte ho:\n",
    "   ```python\n",
    "   from langchain.chat_models import ChatOpenAI\n",
    "   from langchain.schema.messages import HumanMessage\n",
    "\n",
    "   # Initializing the GPT-4V model\n",
    "   chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "   # Passing image to LLM for summary\n",
    "   msg = chat.invoke(\n",
    "       [\n",
    "           HumanMessage(\n",
    "               content=[\n",
    "                   {\"type\": \"text\", \"text\": \"Please give a summary of the image provided.\"},\n",
    "                   {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_str}\"}},\n",
    "               ]\n",
    "           )\n",
    "       ]\n",
    "   )\n",
    "\n",
    "   print(msg.content)  # Summary of the image\n",
    "   ```\n",
    "\n",
    "### Conclusion:\n",
    "Yeh pura process tumhe **PDFs ke analysis** ka ek structured aur reliable tareeqa deta hai jisme tum tables, images, aur text ko extract karke apne data ko LLMs ke liye ready kar sakte ho. Tables ko accurately extract karne ke liye tum advanced methods jaise **YOLOX** ka use karte ho, aur fir multi-modal approaches jaise **GPT-4V** ko integrate karte ho for images aur tables ki summarization ke liye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Multi-Modal (text + images)\n",
    "\n",
    "Is paragraph mein hum **multi-modal text splitting** ke concept ko explore kar rahe hain, jo text ke sath images ko bhi handle karta hai. Iska matlab hai ke hum text aur images ko sath milakar split karte hain aur use karte hain, jo aik evolving field hai. Ye technique LangChain ke Lance Martin ne popular banai thi, aur hum uska ek tareeqa dekhte hain.\n",
    "\n",
    "### PDF ko Process karna aur Images ko Handle karna\n",
    "\n",
    "Yahan hum PDF files ke andar text aur images ko separate karne ka tareeqa dekhte hain.\n",
    "\n",
    "1. **Library Installation**: Pehle hum `unstructured` library install karte hain jo various document formats ko handle karne ke liye use hoti hai, jaise PDF. \n",
    "   \n",
    "   ```python\n",
    "   #!pip3 install \"unstructured[all-docs]\"\n",
    "   ```\n",
    "\n",
    "2. **PDF Partitioning**: Hum `partition_pdf` function ko use kar rahe hain jo PDF ke alag alag elements ko nikalta hai, jisme images aur text donon shamil hain.\n",
    "\n",
    "   ```python\n",
    "   from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "   filepath = \"static/VisualInstruction.pdf\"\n",
    "   raw_pdf_elements = partition_pdf(\n",
    "       filename=filepath,\n",
    "       extract_images_in_pdf=True,  # images ko extract karta hai\n",
    "       infer_table_structure=True,  # tables ke layout ko samajhta hai\n",
    "       chunking_strategy=\"by_title\",  # title ke mutabiq chunks banata hai\n",
    "       max_characters=4000,  # max 4000 characters per chunk\n",
    "       new_after_n_chars=3800,  # naya chunk 3800 characters ke baad banta hai\n",
    "       combine_text_under_n_chars=2000,  # small text ko aggregate karta hai\n",
    "       image_output_dir_path=\"static/pdfImages/\"  # images ko output folder me save karta hai\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Images Extract Karna**: PDF se images extract karne ke baad, images ko folder `static/pdfImages/` me save karte hain. Is example mein images ko split karna scope ke bahar hai, lekin aage hum images ka use karenge.\n",
    "\n",
    "### Images ke Saath kaam karna\n",
    "\n",
    "Ab images ko sirf folder mein rakhne ka fayda nahi, hum images ko process karke kuch meaningful information nikalna chahte hain. Yahan hum GPT-4V (GPT-4 Vision) ka use karenge jo images se summaries ya embeddings banata hai.\n",
    "\n",
    "1. **Image to Base64 Conversion**: Pehle hum image ko Base64 format mein convert karte hain, taake usay model ko pass kiya ja sake.\n",
    "\n",
    "   ```python\n",
    "   from PIL import Image\n",
    "   import base64\n",
    "   import io\n",
    "\n",
    "   def image_to_base64(image_path):\n",
    "       with Image.open(image_path) as image:\n",
    "           buffered = io.BytesIO()\n",
    "           image.save(buffered, format=image.format)\n",
    "           img_str = base64.b64encode(buffered.getvalue())\n",
    "           return img_str.decode('utf-8')\n",
    "\n",
    "   image_str = image_to_base64(\"static/pdfImages/figure-15-6.jpg\")\n",
    "   ```\n",
    "\n",
    "2. **GPT-4 Vision ko Image Dena**: Ab hum GPT-4 Vision model ko image denge aur usay ek descriptive summary banane ke liye kahenge.\n",
    "\n",
    "   ```python\n",
    "   from langchain.chat_models import ChatOpenAI\n",
    "   from langchain.schema.messages import HumanMessage\n",
    "\n",
    "   chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "   msg = chat.invoke(\n",
    "       [\n",
    "           HumanMessage(\n",
    "               content=[\n",
    "                   {\"type\": \"text\", \"text\": \"Please give a summary of the image provided. Be descriptive\"},\n",
    "                   {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_str}\"}}\n",
    "               ]\n",
    "           )\n",
    "       ]\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Summary ka Result**: GPT-4V ne jo summary di, wo kuch is tarah se hai:\n",
    "\n",
    "   ```\n",
    "   'The image shows a baking tray with pieces of fried chicken arranged to roughly mimic the continents on Earth as seen from space...'\n",
    "   ```\n",
    "\n",
    "   Is tarah, image ko process karke ek meaningful summary mil gayi jo aap future mein use kar sakte hain, jaise ke **semantic search** mein.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Is process mein:\n",
    "- Humne PDF se text aur images ko separate kiya.\n",
    "- Images ko Base64 format mein convert kiya.\n",
    "- GPT-4 Vision model ka use karke images ki descriptive summary banayi.\n",
    "\n",
    "Agar aapko ye samajhna hai ke multi-modal text splitting kaise kaam karta hai, to ye technique useful hai jab aap text aur images ko sath process karna chahte hain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
