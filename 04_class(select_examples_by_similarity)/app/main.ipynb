{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance (MMR) Kya Hai?\n",
    "\n",
    "**MMR** ek technique hai jo aapko examples select karne mein madad karti hai. Iska maksad ye hai ke jo examples aap select kar rahe hain, wo aapas mein similar hone ke saath-saath thodi diversity bhi rakhein, yani sab kuch ek jaisa na ho.\n",
    "\n",
    "### Kaise Kaam Karta Hai?\n",
    "\n",
    "1. **Examples**: Aapke paas kuch examples hain, jahan har example ek input aur uska output hota hai. Jaise:\n",
    "   - Input: \"happy\", Output: \"sad\"\n",
    "   - Input: \"tall\", Output: \"short\"\n",
    "\n",
    "2. **Similarity**: MMR ko samajhne ke liye yeh zaroori hai ke hum examples ki similarity ko measure karein. Ye similarity samajhne ke liye hum embeddings ka istemal karte hain. Embeddings basically ek tarah ki numerical representation hoti hain jo humein batati hain ke do words ya phrases kitne similar hain.\n",
    "\n",
    "3. **Diversity**: Jab MMR examples ko select karta hai, to wo ye dekhta hai ke jo naye examples wo select kar raha hai, wo pehle se selected examples se zyada similar na hon. Is tarah se wo diversity ko bhi maintain karta hai.\n",
    "\n",
    "### Example Kaise Kaam Karta Hai:\n",
    "\n",
    "1. **MMR Example Selector Ka Istemal**: Aap MMR example selector ko use karte hain. Isme aapko pehle examples ki list deni hoti hai, phir wo unme se sabse relevant aur diverse examples select karta hai.\n",
    "\n",
    "2. **Input dena**: Jab aap input dete hain, for example \"worried\", MMR dekhta hai ke kaunse examples is input ke liye sabse behtar hain.\n",
    "\n",
    "### Semantic Similarity Selector:\n",
    "\n",
    "Agar aap sirf similarity dekhna chahte hain, to aap **Semantic Similarity Example Selector** use karte hain. Yeh sirf un examples ko select karega jo input se zyada similar hain, bina diversity ki parwah kiye.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **MMR**: Similar aur diverse examples select karta hai.\n",
    "- **Semantic Selector**: Sirf similar examples select karta hai.\n",
    "\n",
    "Agar aapko ab bhi kuch samajh nahi aaya ya koi aur sawal hai, to aap puch sakte hain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diversity ka matlab hai ke selected examples mein variety ho, yaani wo ek dusre se itne similar na hon ke unka output ya meaning bilkul ek jaisa ho. Jab aap examples ko select karte hain, to agar aap har baar sirf wohi examples chunte hain jo ek jaisa hain, to aapke paas koi variety nahi hogi.\n",
    "\n",
    "### MMR mein Diversity Ki Importance:\n",
    "\n",
    "1. **Variety**: MMR ka maksad ye hai ke aapke selected examples mein kuch variety ho, taake jab aap kisi nayi input par kaam karte hain, to aapke paas different perspectives ya options hon.\n",
    "\n",
    "2. **Avoiding Redundancy**: Agar aap sirf similar examples lete hain, to aap kaam ko itna effective nahi bana sakte. Jaise agar aapke paas sirf \"happy\" aur \"joyful\" hon, to inka output bhi same hoga. Lekin agar aap \"happy\" aur \"sad\" lete hain, to ye ek dusre se different hain aur aapko better understanding de sakte hain.\n",
    "\n",
    "3. **Enhanced Learning**: Diverse examples aapko better learning experience dete hain. Jab aapke paas different examples hote hain, to aap unhe compare karke seekhte hain ke kaise alag-alag inputs ke liye alag outputs ho sakte hain.\n",
    "\n",
    "### Example se Samjhana:\n",
    "\n",
    "Agar aap sirf \"happy\" aur \"joyful\" ke examples lete hain:\n",
    "- Input: \"happy\" → Output: \"joyful\"\n",
    "- Input: \"happy\" → Output: \"joyful\"\n",
    "\n",
    "Yahan par aapke paas koi diversity nahi hai, dono outputs bilkul same hain.\n",
    "\n",
    "Lekin agar aap \"happy\" aur \"sad\" lete hain:\n",
    "- Input: \"happy\" → Output: \"sad\"\n",
    "- Input: \"windy\" → Output: \"calm\"\n",
    "\n",
    "Yahan par aapko diversity milti hai, kyunke aapke selected examples alag-alag meanings aur contexts ko represent kar rahe hain.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Diversity ensure karta hai ke aapka learning process aur outputs zyada informative aur useful hon. MMR isliye diversity par focus karta hai, taake aapko behtar results milein aur aap kisi bhi input par alag-alag perspectives se soch sakein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Give the antonym of every input\\n\\nInput: What is the boiling point of water in Celsius?\\nOutput: The boiling point of water is 100°C.\\n\\nInput: What is the capital of France?\\nOutput: The capital of France is Paris.\\n\\nInput: what is standard temprature in the earth?\\nOutput:'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector,  MaxMarginalRelevanceExampleSelector\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load the API key from .env file\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=gemini_api_key,  # Corrected the variable usage\n",
    "    temperature=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Examples for few-shot prompting\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris.\"},\n",
    "    {\"input\": \"Who wrote the play 'Romeo and Juliet'?\", \"output\": \"William Shakespeare wrote 'Romeo and Juliet'.\"},\n",
    "    {\"input\": \"What is the boiling point of water in Celsius?\", \"output\": \"The boiling point of water is 100°C.\"},\n",
    "    {\"input\": \"Who is the current CEO of Tesla?\", \"output\": \"Elon Musk is the current CEO of Tesla.\"}\n",
    "]\n",
    "\n",
    "# Embeddings setup\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=gemini_api_key,\n",
    "    model=\"models/text-embedding-004\",\n",
    ")\n",
    "\n",
    "\n",
    "# Documents to store in vector database\n",
    "documents = [Document(page_content=example['input'], metadata={\"input\": example[\"input\"], \"output\": example[\"output\"]})\n",
    "             for example in examples]\n",
    "\n",
    "# Create a vector store using FAISS\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Example selector using SemanticSimilarityExampleSelector\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vector_store,  \n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# Example selector using MaxMarginalRelevanceExampleSelector\n",
    "# example_selector = MaxMarginalRelevanceExampleSelector(\n",
    "#     vectorstore=vector_store,  \n",
    "#     k=2,\n",
    "# )\n",
    "\n",
    "# Create the prompt template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Dynamic few-shot prompt template\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"],\n",
    "    # verbose=True,\n",
    ")\n",
    "print(dynamic_prompt.invoke(\n",
    "    {\n",
    "        \"input\": \"what is standard temprature in the earth?\",\n",
    "\n",
    "\n",
    "    }\n",
    "))\n",
    "\n",
    "# # Create the LLMChain\n",
    "# # Example code using RunnableSequence\n",
    "# llm_chain = RunnableSequence(dynamic_prompt | llm)\n",
    "\n",
    "\n",
    "# # Run the chain with an example input\n",
    "# chain_input = {\"input\": \"who is the CEO  of Google?\"}\n",
    "\n",
    "\n",
    "# chain_output = llm_chain.invoke(chain_input)\n",
    "\n",
    "# # Print the output of the LLMChain\n",
    "# print(chain_output.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
