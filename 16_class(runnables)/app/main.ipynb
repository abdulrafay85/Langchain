{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to chain runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke's humor relies on a pun.  It's mildly funny.  The setup creates an expectation of a scientific reason for distrust, but the punchline plays on the double meaning of \"make up.\"  It's a simple, somewhat predictable pun, so its humor is subjective. Some people will find it amusing, others less so. The addition about bears is a non sequitur that adds a slightly absurd and unexpected element, which some might find humorous, while others might find it confusing or detracting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries from LangChain\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Loading environment variables (API key for Gemini model)\n",
    "load_dotenv()\n",
    "\n",
    "# Fetching the API key for Gemini from the environment variables\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Setting the API key for use in the environment\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "\n",
    "# Initializing the Google Generative AI model with required parameters\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Specifying the model to use\n",
    "    api_key=gemini_api_key,    # Providing the API key\n",
    "    temperature=0.3            # Setting the temperature for response creativity (0.3 is more deterministic)\n",
    ")\n",
    "\n",
    "# Creating a prompt template for generating a joke about a given topic\n",
    "prompt: HumanMessage = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "# Creating a chain where the prompt is passed to the model and the output is parsed into a string\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Creating a prompt template for analyzing if the joke is funny or not\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "# Composing the full chain of operations:\n",
    "# 1. Generate a joke using the 'joke' chain\n",
    "# 2. Analyze if the joke is funny using the 'analysis_prompt' chain\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Running the composed chain with the input topic \"bears\"\n",
    "res: AIMessage = composed_chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "# Printing the result, which will include the analysis of the joke's funniness\n",
    "print(res)\n",
    "\n",
    "\n",
    "# How to invoke runnables in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to invoke runnables in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, Ukasha is a girl whose school is named Starskise Education.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Importing necessary libraries from LangChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# # Loading environment variables (API key for Gemini model)\n",
    "load_dotenv()\n",
    "\n",
    "# # Fetching the API key for Gemini from the environment variables\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# # Setting the API key for use in the environment\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "\n",
    "# # Initializing the Google Generative AI model with required parameters\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Specifying the model to use\n",
    "    api_key=gemini_api_key,    # Providing the API key\n",
    "    temperature=0.3            # Setting the temperature for response creativity (0.3 is more deterministic)\n",
    ")\n",
    "\n",
    "# Embeddings setup\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=gemini_api_key,\n",
    "    model=\"models/text-embedding-004\",\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"ukasha ek larki hay or is ka school ka name starskise education hay.\"], embedding=embeddings_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# The prompt expects input with keys for \"context\" and \"question\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "res = retrieval_chain.invoke(\"who is ukasha?\")\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
